{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":70203,"databundleVersionId":8068726},{"sourceType":"modelInstanceVersion","sourceId":4893,"databundleVersionId":6683916,"modelInstanceId":2739}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nimport os\n\nimport tensorflow_hub as hub\nimport tensorflow as tf\n\nimport torchaudio\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n\ndf = pd.read_csv('/kaggle/input/birdclef-2024/train_metadata.csv')\nAUDIO_PATH = Path('/kaggle/input/birdclef-2024/train_audio')\n\nmodel_path = 'https://kaggle.com/models/google/bird-vocalization-classifier/frameworks/TensorFlow2/variations/bird-vocalization-classifier/versions/4'\nmodel = hub.load(model_path)\nmodel_labels_df = pd.read_csv(hub.resolve(model_path) + \"/assets/label.csv\")\n\nSAMPLE_RATE = 32000\nWINDOW = 5*SAMPLE_RATE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:00:52.325150Z","iopub.execute_input":"2025-05-21T16:00:52.326107Z","iopub.status.idle":"2025-05-21T16:01:12.727077Z","shell.execute_reply.started":"2025-05-21T16:00:52.326064Z","shell.execute_reply":"2025-05-21T16:01:12.726113Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"index_to_label = sorted(df.primary_label.unique())\nlabel_to_index = {v: k for k, v in enumerate(index_to_label)}\nmodel_labels = {v: k for k, v in enumerate(model_labels_df.ebird2021)}\nmodel_bc_indexes = [model_labels[label] if label in model_labels else -1 for label in index_to_label]\n\n# filter out birds that the model doesn't predict\nmissing_birds = set(np.array(index_to_label)[np.array(model_bc_indexes) == -1])\nmissing_birds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:56:10.633720Z","iopub.execute_input":"2025-05-21T15:56:10.635286Z","iopub.status.idle":"2025-05-21T15:56:10.661624Z","shell.execute_reply.started":"2025-05-21T15:56:10.635222Z","shell.execute_reply":"2025-05-21T15:56:10.660706Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"{'bkrfla1', 'indrol2'}"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Save embeddings and predictions for every 5 sec non-overlapping audio¶\n\n\n# use a torch dataloader to decode audio in parallel on CPU while GPU is running\n\nclass AudioDataset(Dataset):\n    def __len__(self):\n        return len(df)\n    def __getitem__(self, i):\n        filename = df.filename[i]\n        audio = torchaudio.load(AUDIO_PATH / filename)[0].numpy()[0]\n        return audio, filename\n        \ndataloader = DataLoader(AudioDataset(), batch_size=1, num_workers=os.cpu_count())\n\n\n# embeddings are formated like {\"filename\": np.array(nx1280)} \nall_embeddings = {}\n\n# predictiones formated like {\"filename\": np.array(nx264)} \nall_predictions = {}\n\n# (where n = the number of non overlapping 5 sec chunks in the audio)\n\nwith tf.device('/gpu:0'):\n    for audio, filename in tqdm(dataloader):\n        audio = audio[0]\n        filename = filename[0]\n        file_embeddings = []\n        file_predictions = []\n        for i in range(0, len(audio), WINDOW):\n            clip = audio[i:i+WINDOW]\n            if len(clip) < WINDOW:\n                clip = np.concatenate([clip, np.zeros(WINDOW - len(clip))])\n            result = model.infer_tf(clip[None, :])\n            file_embeddings.append(result[1][0].numpy())\n            prediction = np.concatenate([result[0].numpy(), -100], axis=None) # add -100 logit for unpredicted birds\n            file_predictions.append(prediction[model_bc_indexes])\n        all_embeddings[filename] = np.stack(file_embeddings)\n        all_predictions[filename] = np.stack(file_predictions)\n        break\n\ntorch.save(all_embeddings, 'embeddings.pt')\ntorch.save(all_predictions, 'predictions.pt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scores of predictions on the first 5 seconds of each recording¶\n\npredicted_classes = torch.tensor([row[0].argmax() for row in all_predictions.values()])\nactual_classes = torch.tensor([label_to_index[label] for label in df.primary_label])\ncorrect = predicted_classes == actual_classes\naccuracy = correct.float().mean()\naccuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:15:02.441636Z","iopub.execute_input":"2025-05-21T16:15:02.442070Z","iopub.status.idle":"2025-05-21T16:15:02.459987Z","shell.execute_reply.started":"2025-05-21T16:15:02.442045Z","shell.execute_reply":"2025-05-21T16:15:02.458949Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"tensor(0.0043)"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}